{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe6c0e66",
   "metadata": {},
   "source": [
    "# Kanji Recognition with CNN\n",
    "**Author:** Thomas K/BIDI  \n",
    "**Description:** This notebook aims to train a model for Kanji classification on the 500 most frequently used Kanji using CNN and TripletLoss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e116cb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import decode_image, read_file\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "import json\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0579c119",
   "metadata": {},
   "source": [
    "## Kanji Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906e3943",
   "metadata": {},
   "source": [
    "We're going to use the dataset from [davidluzgouveia/kanji-data](https://github.com/davidluzgouveia/kanji-data).  \n",
    "From it, we will extract the 500 most frequent kanjis, in order, and their furiganas and meanings, these kanji will be used to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25426d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"kanji.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    kanji_json = json.load(f)\n",
    "\n",
    "kanji500 = [i for i in range(500)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4114b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (key, value) in kanji_json.items():\n",
    "    if (value[\"freq\"] != None and value[\"freq\"] <= 500):\n",
    "        kanji500[value[\"freq\"]-1] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a954349",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for kanji in kanji500:\n",
    "    rows.append({\n",
    "        \"Kanji\": kanji,\n",
    "        \"Furigana\": \" \".join(kanji_json[kanji][\"readings_kun\"]),\n",
    "        \"Meaning\": \" | \".join(kanji_json[kanji][\"meanings\"])\n",
    "    })\n",
    "df = pd.DataFrame(rows)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac2f679",
   "metadata": {},
   "source": [
    "## Triplet Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bf3eef",
   "metadata": {},
   "source": [
    "Let's make a small introduction to **Triplet Loss**.  \n",
    "The **Triplet Loss** is used in *metric learning* and mostly for *one-shot learning*, our objective is to create *embeddings* of handwritten kanji images where, in the *latent* space, we have :\n",
    " - Similar images which are close to each other\n",
    " - Different images which are far from each other\n",
    "\n",
    "This is where the **Triplet Loss** is useful, we will consider this 3 elements :\n",
    " - **Anchor (A)** : a reference element\n",
    " - **Positive (P)** : an example of the same class as the Anchor\n",
    " - **Negative (N)** : an example of another class\n",
    "\n",
    "We use the network to obtain our embeddings.  \n",
    "For an input $x$ we will call $f(x) \\in \\mathbb{R}^d$ its embedding.\n",
    "\n",
    "We aim to establish the following inequality :  \n",
    "$||f(A)-f(P)||^2 < ||f(A)-f(N)||^2$  \n",
    "\n",
    "The objective is to make embedding of the same class closer than those from different classes.\n",
    "\n",
    "We introduce $\\alpha$, a margin which will represent the minimum distance between two classes.  \n",
    "This lead us to the following formulation of the **Triplet Loss**:  \n",
    "$\\mathcal{L}(A,P,N)=max(0, ||f(A)-f(P)||^2 - ||f(A)-f(N)||^2 + \\alpha)$  \n",
    "\n",
    "Which will be minimized when $||f(A)-f(P)||^2 + \\alpha \\le ||f(A)-f(N)||^2$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e132496",
   "metadata": {},
   "source": [
    "We will start by generating a certain number of triplet (A, P, N) for each of our 500 kanji, since $Anchor=Positive$ we will only work with the couple (AnchorPositive, Negative) in the future even if its mention as triplet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9827024c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_triplet(df, kanji_column, nb_couple_by_char=5):\n",
    "    \"\"\"\n",
    "    @brief Generate the couples (AnchorPositive, Negative) for the Triplet Loss.\n",
    "    @details This function return a list of nb_couple_by_char couples by char as (AnchorPositive, Negative).\n",
    "    @param df The dataframe.\n",
    "    @param kanji_column The name of the column with the chars.\n",
    "    @param nb_couple_by_char The number of \"triplet\" which will be create for each char.\n",
    "    \"\"\"\n",
    "    triplets = []\n",
    "    len_chars = len(df.loc[:, kanji_column])\n",
    "    for i in range(len_chars):\n",
    "        for j in range(nb_couple_by_char):\n",
    "            rand_idx = i\n",
    "\n",
    "            # Avoid having triplet of same values\n",
    "            while df.loc[rand_idx, kanji_column] == df.loc[i, kanji_column]: \n",
    "                rand_idx = np.random.randint(0, len_chars)\n",
    "\n",
    "            triplets.append((df.loc[i, kanji_column], df.loc[rand_idx, kanji_column]))\n",
    "    return triplets\n",
    "\n",
    "triplet = generate_triplet(df, \"Kanji\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6a86cc",
   "metadata": {},
   "source": [
    "## Loading Handwritten Kanji images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e413979",
   "metadata": {},
   "source": [
    "In this work, we will use the ETL Character Database provided by the Electrotechnical Laboratory (ETL) from Japan. (ETL1->ETL9b)  \n",
    "I cannot share this data with you directly, but you can go on their [website](etlcdb.db.aist.go.jp) and download it yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d9221e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subfolders(path):\n",
    "    \"\"\"\n",
    "    @brief Return a dict with every path's subfolders as keys and for each of them a list of its subfolders.\n",
    "    @param path The path for the root directory in which your subfolders of ETLX are located.\n",
    "    \"\"\"\n",
    "    folders = [name for name in os.listdir(path) if os.path.isdir(os.path.join(path, name))]\n",
    "    subfolders = {}\n",
    "    for folder in folders:\n",
    "        path_to_folder = path + folder + \"/\"\n",
    "        subfolders[folder] = [path_to_folder+name for name in os.listdir(path_to_folder) if os.path.isdir(os.path.join(path_to_folder, name))]\n",
    "    return subfolders\n",
    "\n",
    "subfolders = get_subfolders(\"datasets/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47363e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dictionary(subfolders):\n",
    "    \"\"\"\n",
    "    @brief Load the dictionary of each subfolder.\n",
    "    @details The return value is a dictionary with the subfolders' names as keys and a dataframe with the data of the subfolder as value.\n",
    "    @param subfolders The path towars the subfolders.\n",
    "    \"\"\"\n",
    "    dictionary = {}\n",
    "    for _, subpacks in subfolders.items():\n",
    "        for subfolder in subpacks:\n",
    "            dictionary[subfolder] = pd.read_csv(subfolder+\"/meta.csv\")\n",
    "    return dictionary\n",
    "dictionary = get_dictionary(subfolders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d43071f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kanji_paths(df, kanji_column, dictionary):\n",
    "    \"\"\"\n",
    "    @brief Return a dictionary linking a kanji to the paths of all its handwritten images.\n",
    "    @param df The dataframe with the Kanjis to search for.\n",
    "    @param kanji_column The column with the kanjis.\n",
    "    @param dictionary The dictionary with the information of each subfolders.\n",
    "    \"\"\"\n",
    "    kanji_paths = {}\n",
    "    for kanji in df[kanji_column]:\n",
    "        for path, index_df in dictionary.items():\n",
    "            # Check if the kanji exist in the dataframe\n",
    "            kanji_exist = index_df[\"char\"].isin([kanji]).any() \n",
    "            if kanji_exist:\n",
    "                kanji_paths[kanji] = []\n",
    "                max_size = len(str(index_df[-1::].index[0])) # Get the nb chars of the max len filename\n",
    "                column = index_df.loc[(index_df[\"char\"] == kanji)]\n",
    "                for idx in column.index:\n",
    "                    # We add the path towards this file in the kanji_paths conversion list\n",
    "                    kanji_paths[kanji].append(f\"{path}/{str(idx).zfill(max_size)}.png\")\n",
    "    \n",
    "    return kanji_paths\n",
    "kanji_paths = get_kanji_paths(df, \"Kanji\", dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4af7bfe",
   "metadata": {},
   "source": [
    "## Triplet's custom dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2fad3b",
   "metadata": {},
   "source": [
    "We are creating a function to split the links to the images between training and test datasets, and we create a custom `TripletDataset` class to handle this specific usage of triplets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6132c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_images_paths(images_paths, split_ratio=0.8, seed=None):\n",
    "    \"\"\"\n",
    "    @brief Return the train and test images_path according to the given split_ratio and seed.\n",
    "    @param images_paths The dictionary linking the char (kanji) to the paths of all its handwritten images.\n",
    "    @param split_ratio The desired split_ratio for training and testing.\n",
    "    @param seed Optional seed for shuffling and splitting. None means random.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    train_paths, test_paths = {}, {}\n",
    "\n",
    "    for cls, paths in images_paths.items():\n",
    "        n = len(paths)\n",
    "        split_idx = int(n * split_ratio)\n",
    "        paths = paths.copy()\n",
    "        rng.shuffle(paths)\n",
    "\n",
    "        train_paths[cls] = paths[:split_idx]\n",
    "        test_paths[cls]  = paths[split_idx:]\n",
    "\n",
    "    return train_paths, test_paths\n",
    "\n",
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, triplet, images_paths, transform=None):\n",
    "        # Triplets are represented as couple (AnchorPositive, Negative)\n",
    "        self.triplet = triplet\n",
    "        self.transform = transform\n",
    "        self.images_paths = images_paths\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        anchor_char = positive_char = triplet[index][0]\n",
    "        negative_char = triplet[index][1]\n",
    "\n",
    "        # Load the images in GRAYSCALE\n",
    "        anchor = decode_image(read_file(np.random.choice(self.images_paths[anchor_char])), torchvision.io.ImageReadMode.GRAY)\n",
    "        positive = decode_image(read_file(np.random.choice(self.images_paths[positive_char])), torchvision.io.ImageReadMode.GRAY)\n",
    "        negative = decode_image(read_file(np.random.choice(self.images_paths[negative_char])), torchvision.io.ImageReadMode.GRAY)\n",
    "        \n",
    "        # Apply the transforms\n",
    "        if self.transform:\n",
    "            anchor = self.transform(anchor)\n",
    "            positive = self.transform(positive)\n",
    "            negative = self.transform(negative)\n",
    "\n",
    "        return anchor, positive, negative\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.triplet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63b6622",
   "metadata": {},
   "source": [
    "## Function to train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da25ab7",
   "metadata": {},
   "source": [
    "In this part, we define a series of functions that will be used to train and test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88870c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(batch_size, triplet, images_path, seed=None):\n",
    "    \"\"\"\n",
    "    @brief Create DataLoaders for training, validation, and test using triplets and a given images_path split.\n",
    "    @details Return the training, validation and testing loaders.\n",
    "    @param batch_size The batch size for the DataLoaders.\n",
    "    @param triplet The list of triplets to use (actually couples (AnchorPositive, Negative)).\n",
    "    @param images_path The dictionary linking each class (kanji) to the paths of all its handwritten images.\n",
    "    @param seed Optional seed for shuffling and splitting. None means random.\n",
    "    \"\"\"\n",
    "    # Transform for the black background images\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ConvertImageDtype(torch.float32),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "    ])\n",
    "\n",
    "    # 80% Training, 20% test data\n",
    "    train_images, test_images = split_images_paths(images_path, 0.8, seed)\n",
    "    \n",
    "    # Instanciation of the TripleDataset\n",
    "    training_data = TripletDataset(triplet, train_images, transform)\n",
    "    test_data = TripletDataset(triplet, test_images, transform)\n",
    "\n",
    "    num_train = len(training_data)\n",
    "    # 20% for validation\n",
    "    val_size = int(num_train * 0.2)\n",
    "    train_size = num_train - val_size\n",
    "\n",
    "    used_generator = torch.Generator().manual_seed(seed) if seed is not None else None\n",
    "\n",
    "    # Get the training and validation data\n",
    "    training_data, validation_data = torch.utils.data.random_split(training_data, [train_size, val_size], generator=used_generator)\n",
    "\n",
    "    # Initialization of the dataloader\n",
    "    train_loader = torch.utils.data.DataLoader(training_data, batch_size, shuffle=True, generator=used_generator)\n",
    "    validation_loader = torch.utils.data.DataLoader(validation_data, batch_size, shuffle=False)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, validation_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13102563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(net, learning_rate, weight_decay):\n",
    "    \"\"\"\n",
    "    @brief Create an Adam optimizer for the given network with specified learning rate and weight decay.\n",
    "    @param net The neural network whose parameters will be optimized.\n",
    "    @param learning_rate The learning rate for the optimizer.\n",
    "    @param weight_decay The weight decay (L2 regularization) factor.\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9eff0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cost_function(margin=0.2):\n",
    "    \"\"\"\n",
    "    @brief Return the TripletMarginLoss cost function with the given margin.\n",
    "    @details We use the euclidean distance (p=2).\n",
    "    @param margin The margin parameter.\n",
    "    \"\"\"\n",
    "    cost_function = torch.nn.TripletMarginLoss(margin=margin, p=2)\n",
    "    return cost_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b3d6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data_loader, optimizer, cost_function, device='cuda:0'):\n",
    "    \"\"\"\n",
    "    @brief Train the network for one epoch using the provided DataLoader, optimizer, and loss function.\n",
    "    @param net The neural network to train.\n",
    "    @param data_loader DataLoader providing batches of triplets.\n",
    "    @param optimizer The optimizer used to update the network parameters.\n",
    "    @param cost_function The loss function used to compute the triplet loss.\n",
    "    @param device The device on which to run the computations.\n",
    "    \"\"\"\n",
    "    samples = 0.\n",
    "    cumulative_loss = 0.\n",
    "    correct = 0\n",
    "    \n",
    "    net.train() # Set the network to training mode\n",
    "\n",
    "    for anchor, positive, negative in data_loader:\n",
    "        anchor = anchor.to(device)\n",
    "        positive = positive.to(device)\n",
    "        negative = negative.to(device)\n",
    "\n",
    "        # Get the embeddings of each images of the triplet\n",
    "        emb_a = net(anchor)\n",
    "        emb_p = net(positive)\n",
    "        emb_n = net(negative)\n",
    "\n",
    "        # Use them to compute the loss\n",
    "        loss = cost_function(emb_a, emb_p, emb_n)\n",
    "\n",
    "        # Backpropagation phase\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Triplet accuracy\n",
    "            # We compute the euclidean distance between both Anchor-Positive and Anchor-Negative\n",
    "            # If the Anchor is closer to the Positive, we consider that the model did a good classification\n",
    "            # We'll use that score to compute accuracy\n",
    "        dist_pos = (emb_a - emb_p).pow(2).sum(dim=1)\n",
    "        dist_neg = (emb_a - emb_n).pow(2).sum(dim=1)\n",
    "        correct += (dist_pos < dist_neg).sum().item()\n",
    "\n",
    "        # Compute the weighted average loss per batch and add it to the cumulative loss\n",
    "        batch_size = anchor.size(0)\n",
    "        samples += batch_size\n",
    "        cumulative_loss += loss.item() * batch_size\n",
    "\n",
    "    # Return the mean loss and the accuracy\n",
    "    mean_loss = cumulative_loss / samples\n",
    "    accuracy = correct / samples\n",
    "    return mean_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db219edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, data_loader, cost_function, device='cuda:0'):\n",
    "    \"\"\"\n",
    "    @brief Evaluate the network for one epoch using the provided DataLoader and loss function.\n",
    "    @param net The neural network to evaluate.\n",
    "    @param data_loader DataLoader providing batches of triplets.\n",
    "    @param cost_function The loss function used to compute the triplet loss.\n",
    "    @param device The device on which to run the computations.\n",
    "    \"\"\"\n",
    "    samples = 0.\n",
    "    cumulative_loss = 0.\n",
    "    correct = 0\n",
    "\n",
    "    net.eval()  # Set the network to evaluation mode\n",
    "\n",
    "    # Disable gradient computation\n",
    "    with torch.no_grad():\n",
    "        for anchor, positive, negative in data_loader:\n",
    "            anchor = anchor.to(device)\n",
    "            positive = positive.to(device)\n",
    "            negative = negative.to(device)\n",
    "\n",
    "            # Get embeddings\n",
    "            emb_a = net(anchor) # shape [batch_size, embedding_dim]\n",
    "            emb_p = net(positive) # shape [batch_size, embedding_dim]\n",
    "            emb_n = net(negative) # shape [batch_size, embedding_dim]\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = cost_function(emb_a, emb_p, emb_n)\n",
    "\n",
    "            # Triplet accuracy\n",
    "                # We compute the euclidean distance between both Anchor-Positive and Anchor-Negative\n",
    "                # If the Anchor is closer to the Positive, we consider that the model did a good classification\n",
    "                # We'll use that score to compute accuracy\n",
    "            dist_pos = (emb_a - emb_p).pow(2).sum(dim=1)\n",
    "            dist_neg = (emb_a - emb_n).pow(2).sum(dim=1)\n",
    "            correct += (dist_pos < dist_neg).sum().item()\n",
    "\n",
    "            # Compute the weighted average loss per batch and add it to the cumulative loss\n",
    "            batch_size = anchor.size(0)\n",
    "            samples += batch_size\n",
    "            cumulative_loss += loss.item() * batch_size\n",
    "\n",
    "    # Return the mean loss and the accuracy\n",
    "    mean_loss = cumulative_loss / samples\n",
    "    accuracy = correct / samples\n",
    "    return mean_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb94b5a4",
   "metadata": {},
   "source": [
    "The **accuracy** is just *informative*, it is not a good metric here.  \n",
    "Since we consider that when the inequality $||f(A)-f(P)||^2 < ||f(A)-f(N)||^2$ holds, the model has made a correct classification and this is not strictly true.  \n",
    "However, it gives us an idea about the validity of this inequality across the epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c3083b",
   "metadata": {},
   "source": [
    "### EfficientNet as the backbone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2614d125",
   "metadata": {},
   "source": [
    "We will use [EfficientNet](https://en.wikipedia.org/wiki/EfficientNet) as the backbone of our model by adding a new final layer to produce 128-dimensional embeddings, and by modifying the input to accept GRAYSCALE images.  \n",
    "You can choose to set the *pretrained* parameter to *True* or *False*, depending on whether you want to use the EfficientNet pretrained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be60a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNetEmbedding(torch.nn.Module):\n",
    "    def __init__(self, model_name='tf_efficientnetv2_s', embedding_dim=128):\n",
    "        super().__init__()\n",
    "\n",
    "        # Load the model without the last layer\n",
    "        self.backbone = timm.create_model(model_name, pretrained=True, num_classes=0)\n",
    "\n",
    "        # Modification of the first layer to accept GRAYSCALE images as input\n",
    "        first_conv = self.backbone.conv_stem\n",
    "        self.backbone.conv_stem = torch.nn.Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=first_conv.out_channels,\n",
    "            kernel_size=first_conv.kernel_size,\n",
    "            stride=first_conv.stride,\n",
    "            padding=first_conv.padding,\n",
    "            bias=first_conv.bias is not None\n",
    "        )\n",
    "        # Apply a mean of the 3 canals weights for the new 1 canal layer we got\n",
    "        self.backbone.conv_stem.weight = torch.nn.Parameter(first_conv.weight.mean(dim=1, keepdim=True))\n",
    "\n",
    "        # Addition of an embedding layer as the last layer\n",
    "        self.embedding = torch.nn.Linear(self.backbone.num_features, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # We use the backbone\n",
    "        x = self.backbone(x)\n",
    "        # L2 regularization of the embedding\n",
    "        x = F.normalize(self.embedding(x), p=2, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adc6de1",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af132bde",
   "metadata": {},
   "source": [
    "Here, we have the actual training of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128b04ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(batch_size=128,\n",
    "         device=('cuda:0' if torch.cuda.is_available() else 'cpu'),\n",
    "         learning_rate=0.001,\n",
    "         weight_decay=1e-6,\n",
    "         epochs=50,\n",
    "         save_interval=50):\n",
    "    \"\"\"\n",
    "    @brief Train and evaluate EfficientNetEmbedding using triplet loss.\n",
    "    @param batch_size Batch size for DataLoaders.\n",
    "    @param device Device to use ('cuda' or 'cpu').\n",
    "    @param learning_rate Learning rate for the optimizer.\n",
    "    @param weight_decay Weight decay for the optimizer.\n",
    "    @param epochs Number of training epochs.\n",
    "    @param save_interval Save the model every N epochs. The name will be net_{current_epoch}.pth\n",
    "    \"\"\"\n",
    "    # Load datasets\n",
    "    train_loader, val_loader, test_loader = get_data(batch_size, triplet, kanji_paths)\n",
    "\n",
    "    # Initialize network, optimizer, and loss function\n",
    "    net = EfficientNetEmbedding().to(device)\n",
    "    optimizer = get_optimizer(net, learning_rate, weight_decay)\n",
    "    cost_function = get_cost_function(margin=0.2)\n",
    "\n",
    "    # Evaluation before training\n",
    "    train_loss, train_acc = test(net, train_loader, cost_function, device)\n",
    "    val_loss, val_acc = test(net, val_loader, cost_function, device)\n",
    "    test_loss, test_acc = test(net, test_loader, cost_function, device)\n",
    "    \n",
    "    # Print the model's evaluation metrics, before training\n",
    "    print(\"Before training :\")\n",
    "    print(f\"\\t Training loss : {train_loss:.5f} | Training triplet accuracy : {train_acc:.3f}\")\n",
    "    print(f\"\\t Validation loss {val_loss:.5f} | Validation triplet accuracy : {val_acc:.3f}\")\n",
    "    print(f\"\\t Test loss {test_loss:.5f} | Test triplet accuracy : {test_acc:.3f}\")\n",
    "    print(\"-\"*50)\n",
    "\n",
    "    # Training loop\n",
    "    for e in range(epochs):\n",
    "        # Save every save_interval epochs\n",
    "        if ((e + 1) % save_interval == 0):\n",
    "            torch.save(net.state_dict(), f\"net_{e+1}.pth\")\n",
    "\n",
    "        # Train one epoch\n",
    "        train_loss, train_acc = train(net, train_loader, optimizer, cost_function, device)\n",
    "\n",
    "        # Evaluate on validation set with triplet accuracy\n",
    "        val_loss, val_acc = test(net, val_loader, cost_function, device)\n",
    "\n",
    "        # Print epoch summary\n",
    "        print(f\"Epoch {e+1}/{epochs} | \"\n",
    "            f\"Train loss: {train_loss:.5f} | \"\n",
    "            f\"Train triplet accuracy: {train_acc:.3f} | \"\n",
    "            f\"Validation loss: {val_loss:.5f} | \"\n",
    "            f\"Validation triplet accuracy: {val_acc:.3f}\")\n",
    "        print(\"-\"*50)\n",
    "\n",
    "\n",
    "    # Final evaluation after training\n",
    "    train_loss, train_acc = test(net, train_loader, cost_function, device)\n",
    "    val_loss, val_acc = test(net, val_loader, cost_function, device)\n",
    "    test_loss, test_acc = test(net, test_loader, cost_function, device)\n",
    "\n",
    "    # Print the model's evaluation metrics, after training\n",
    "    print(\"After training :\")\n",
    "    print(f\"\\t Training loss : {train_loss:.5f} | Training triplet accuracy : {train_acc:.3f}\")\n",
    "    print(f\"\\t Validation loss {val_loss:.5f} | Validation triplet accuracy : {val_acc:.3f}\")\n",
    "    print(f\"\\t Test loss {test_loss:.5f} | Test triplet accuracy : {test_acc:.3f}\")\n",
    "    print(\"-\"*50)\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9793f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e709044",
   "metadata": {},
   "source": [
    "With my RTX 2070, I run 50 epochs in **~390min (~6h30)**.  \n",
    "For a *small number of kanji*, you could reduce the size of the input images in the transform, a lower resolution will *decrease training time*.  \n",
    "I have already trained a model on 75 kanjis using a resolution of *64x64*, and I used it to predict the 10 most probable kanji from a drawing.  \n",
    "However, for a large amount of complex kanji, this resolution is not enough."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cfa78d",
   "metadata": {},
   "source": [
    "### Utilization functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0581913",
   "metadata": {},
   "source": [
    "These are functions to utilize the model in real-world use cases, you can transform the common white background images into a compatible format with the model, obtain their embeddings and compute the cosine similarity with a set of reference vectors (which consists of embeddings of references images for each kanjis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54ff874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform for the white background images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: 1.0 - x),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fedfec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path, transform, device):\n",
    "    \"\"\"\n",
    "    @brief Load an image, apply the transform on it and return its tensor.\n",
    "    @param path The image's path, supposed valid.\n",
    "    @param transform The transform to apply to the images.\n",
    "    @param device The device on which the tensor will be loaded.\n",
    "    \"\"\"\n",
    "    image = Image.open(path).convert('L')\n",
    "    # shape [1, 1, H, W]\n",
    "    tensor = transform(image).unsqueeze(0).to(device)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a02f675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(net, image_tensor):\n",
    "    \"\"\"\n",
    "    @brief Return the embedding of a tensor according the given net.\n",
    "    @param net The net use for embedding.\n",
    "    @param image_tensor An image's tensor.\n",
    "    \"\"\"\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        embedding = net(image_tensor)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5843fb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(embedding, reference_vectors):\n",
    "    \"\"\"\n",
    "    @brief Compute the cosine similarity between the given embedding and all reference vectors.\n",
    "    @param embedding The image embedding (tensor of shape [1, embedding_dim]).\n",
    "    @param reference_vectors Tensor containing reference embeddings (shape [num_refs, embedding_dim]).\n",
    "    \"\"\"\n",
    "    embedding = F.normalize(embedding, p=2, dim=1)\n",
    "    reference_vector = F.normalize(reference_vectors, p=2, dim=1)\n",
    "\n",
    "    sims = torch.mm(embedding, reference_vector.T)\n",
    "    return sims.squeeze(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
